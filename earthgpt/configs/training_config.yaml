# EarthGPT Training Configuration

training:
  # Paths
  output_dir: "./outputs/earthgpt-lora"
  logging_dir: "./logs"
  cache_dir: "./cache"

  # Data
  data_path: "./data/unified_train.jsonl"
  val_data_path: "./data/unified_val.jsonl"
  image_root: "./data/images"

  # Training Hyperparameters
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # Effective batch size: 32

  # Learning Rate
  learning_rate: 2e-4
  warmup_steps: 100
  lr_scheduler_type: "cosine"

  # Optimization
  optim: "paged_adamw_8bit"
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Mixed Precision
  fp16: false
  bf16: true

  # Saving
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3

  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 500

  # Logging
  logging_steps: 10
  report_to: "wandb"

  # DeepSpeed (Optional)
  deepspeed: null  # Set to "configs/ds_config.json" if using DeepSpeed

  # Other
  seed: 42
  dataloader_num_workers: 4
  remove_unused_columns: false
  group_by_length: false

# Task Mixture Ratios (sampling probabilities)
task_mixture:
  obb: 0.4
  vqa: 0.3
  captioning: 0.3
