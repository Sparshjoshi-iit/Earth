# EarthGPT Model Configuration

model:
  # Vision Encoder
  vision_encoder:
    name: "google/siglip-so400m-patch14-384"
    freeze: true
    hidden_size: 1152
    image_size: 384

  # Language Model
  language_model:
    name: "meta-llama/Llama-3.2-3B-Instruct"
    freeze: false  # Will use LoRA
    hidden_size: 3072

  # Projector
  projector:
    type: "mlp"
    input_dim: 1152
    hidden_dim: 3072
    output_dim: 3072
    num_layers: 2

  # Special Tokens
  special_tokens:
    image_token: "<image>"
    pad_token: "<pad>"

# LoRA Configuration
lora:
  r: 64
  lora_alpha: 128
  target_modules:
    - "q_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization (Q-LoRA)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
